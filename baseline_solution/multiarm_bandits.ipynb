{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517abd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import typing as t\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import HTML\n",
    "from dataclasses import dataclass\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def dataframe_to_html(df: pl.DataFrame, columns: list) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Polars DataFrame to an HTML table with specified columns.\n",
    "\n",
    "    Args:\n",
    "        df: Polars DataFrame containing the data.\n",
    "        columns: List of column names to include in the HTML table.\n",
    "\n",
    "    Returns:\n",
    "        str: HTML string representing the table.\n",
    "    \"\"\"\n",
    "    html = [\"<table border='1'>\"]\n",
    "\n",
    "    html.append(\"<tr>\")\n",
    "    for col in columns:\n",
    "        html.append(f\"<th>{col}</th>\")\n",
    "    html.append(\"</tr>\")\n",
    "\n",
    "    for row in df.iter_rows(named=True):\n",
    "        html.append(\"<tr>\")\n",
    "        for col in columns:\n",
    "            value = row[col]\n",
    "            if col == \"image_url\":\n",
    "                html.append(f\"<td><img src='{value}' width='100' /></td>\")\n",
    "            else:\n",
    "                html.append(f\"<td>{value}</td>\")\n",
    "        html.append(\"</tr>\")\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "    return \"\\n\".join(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590ecc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "(11971437, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>item_id</th><th>is_read</th><th>rating</th><th>date_added</th></tr><tr><td>str</td><td>i64</td><td>bool</td><td>i64</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>&quot;90998a29029ecb1fbeedaff2b71752…</td><td>8473</td><td>false</td><td>0</td><td>2014-05-15 12:51:13</td></tr><tr><td>&quot;90998a29029ecb1fbeedaff2b71752…</td><td>20060</td><td>false</td><td>0</td><td>2014-05-15 12:51:14</td></tr><tr><td>&quot;90998a29029ecb1fbeedaff2b71752…</td><td>8354</td><td>false</td><td>0</td><td>2014-05-15 12:51:17</td></tr><tr><td>&quot;90998a29029ecb1fbeedaff2b71752…</td><td>11575</td><td>false</td><td>0</td><td>2014-05-15 12:51:19</td></tr><tr><td>&quot;90998a29029ecb1fbeedaff2b71752…</td><td>11861</td><td>false</td><td>0</td><td>2014-05-15 12:51:20</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌─────────────────────────────────┬─────────┬─────────┬────────┬─────────────────────┐\n",
       "│ user_id                         ┆ item_id ┆ is_read ┆ rating ┆ date_added          │\n",
       "│ ---                             ┆ ---     ┆ ---     ┆ ---    ┆ ---                 │\n",
       "│ str                             ┆ i64     ┆ bool    ┆ i64    ┆ datetime[μs]        │\n",
       "╞═════════════════════════════════╪═════════╪═════════╪════════╪═════════════════════╡\n",
       "│ 90998a29029ecb1fbeedaff2b71752… ┆ 8473    ┆ false   ┆ 0      ┆ 2014-05-15 12:51:13 │\n",
       "│ 90998a29029ecb1fbeedaff2b71752… ┆ 20060   ┆ false   ┆ 0      ┆ 2014-05-15 12:51:14 │\n",
       "│ 90998a29029ecb1fbeedaff2b71752… ┆ 8354    ┆ false   ┆ 0      ┆ 2014-05-15 12:51:17 │\n",
       "│ 90998a29029ecb1fbeedaff2b71752… ┆ 11575   ┆ false   ┆ 0      ┆ 2014-05-15 12:51:19 │\n",
       "│ 90998a29029ecb1fbeedaff2b71752… ┆ 11861   ┆ false   ┆ 0      ┆ 2014-05-15 12:51:20 │\n",
       "└─────────────────────────────────┴─────────┴─────────┴────────┴─────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder = \"/home/gleb_galagan/tbank_recsys/sirius_recsys/sirius-2025-recsys/data/\"\n",
    "train = pl.read_parquet(data_folder + \"train.pq\")\n",
    "print(\"Train:\")\n",
    "print(train.shape)\n",
    "display(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8de2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books:\n",
      "(34322, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>item_id</th><th>series</th><th>tags</th><th>title</th><th>description</th><th>url</th><th>image_url</th><th>authors</th></tr><tr><td>i64</td><td>list[str]</td><td>list[str]</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td></tr></thead><tbody><tr><td>0</td><td>[null]</td><td>[&quot;e-book&quot;, &quot;young-adult&quot;, … &quot;y-a&quot;]</td><td>&quot;Hallie Hath No Fury . . .&quot;</td><td>&quot;There are two sides to every s…</td><td>&quot;https://www.goodreads.com/book…</td><td>&quot;https://images.gr-assets.com/b…</td><td>[{&quot;1879494&quot;,&quot;&quot;}]</td></tr><tr><td>1</td><td>[&quot;149079&quot;]</td><td>[&quot;primary&quot;, &quot;melissa-j--morgan&quot;, … &quot;fiction&quot;]</td><td>&quot;Hide and Shriek: Super Special…</td><td>&quot;The girls go on an overnight a…</td><td>&quot;https://www.goodreads.com/book…</td><td>&quot;https://s.gr-assets.com/assets…</td><td>[{&quot;21740&quot;,&quot;&quot;}]</td></tr><tr><td>2</td><td>[null]</td><td>[&quot;friendship&quot;, &quot;middle-reader&quot;, … &quot;my-library&quot;]</td><td>&quot;Dear Mom, You&#x27;re Ruining My Li…</td><td>&quot;Samantha Slayton worries about…</td><td>&quot;https://www.goodreads.com/book…</td><td>&quot;https://s.gr-assets.com/assets…</td><td>[{&quot;18946&quot;,&quot;&quot;}]</td></tr><tr><td>3</td><td>[&quot;151088&quot;]</td><td>[&quot;summer-2017&quot;, &quot;bullying&quot;, … &quot;re-read&quot;]</td><td>&quot;Bratfest at Tiffany&#x27;s (Clique …</td><td>&quot;Massie Block: The Briarwood bo…</td><td>&quot;https://www.goodreads.com/book…</td><td>&quot;https://images.gr-assets.com/b…</td><td>[{&quot;4605&quot;,&quot;&quot;}]</td></tr><tr><td>4</td><td>[&quot;812067&quot;]</td><td>[&quot;rosemary-vernon&quot;, &quot;young-adult&quot;, … &quot;to-read&quot;]</td><td>&quot;Questions of Love (Sweet Dream…</td><td>&quot;When Sammi Edwards is chosen t…</td><td>&quot;https://www.goodreads.com/book…</td><td>&quot;https://images.gr-assets.com/b…</td><td>[{&quot;792676&quot;,&quot;&quot;}]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ item_id ┆ series     ┆ tags       ┆ title      ┆ descriptio ┆ url        ┆ image_url ┆ authors   │\n",
       "│ ---     ┆ ---        ┆ ---        ┆ ---        ┆ n          ┆ ---        ┆ ---       ┆ ---       │\n",
       "│ i64     ┆ list[str]  ┆ list[str]  ┆ str        ┆ ---        ┆ str        ┆ str       ┆ list[stru │\n",
       "│         ┆            ┆            ┆            ┆ str        ┆            ┆           ┆ ct[2]]    │\n",
       "╞═════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 0       ┆ [null]     ┆ [\"e-book\", ┆ Hallie     ┆ There are  ┆ https://ww ┆ https://i ┆ [{\"187949 │\n",
       "│         ┆            ┆ \"young-adu ┆ Hath No    ┆ two sides  ┆ w.goodread ┆ mages.gr- ┆ 4\",\"\"}]   │\n",
       "│         ┆            ┆ lt\", … \"y… ┆ Fury . . . ┆ to every   ┆ s.com/book ┆ assets.co ┆           │\n",
       "│         ┆            ┆            ┆            ┆ s…         ┆ …          ┆ m/b…      ┆           │\n",
       "│ 1       ┆ [\"149079\"] ┆ [\"primary\" ┆ Hide and   ┆ The girls  ┆ https://ww ┆ https://s ┆ [{\"21740\" │\n",
       "│         ┆            ┆ , \"melissa ┆ Shriek:    ┆ go on an   ┆ w.goodread ┆ .gr-asset ┆ ,\"\"}]     │\n",
       "│         ┆            ┆ -j--morgan ┆ Super      ┆ overnight  ┆ s.com/book ┆ s.com/ass ┆           │\n",
       "│         ┆            ┆ …          ┆ Special…   ┆ a…         ┆ …          ┆ ets…      ┆           │\n",
       "│ 2       ┆ [null]     ┆ [\"friendsh ┆ Dear Mom,  ┆ Samantha   ┆ https://ww ┆ https://s ┆ [{\"18946\" │\n",
       "│         ┆            ┆ ip\", \"midd ┆ You're     ┆ Slayton    ┆ w.goodread ┆ .gr-asset ┆ ,\"\"}]     │\n",
       "│         ┆            ┆ le-reader\" ┆ Ruining My ┆ worries    ┆ s.com/book ┆ s.com/ass ┆           │\n",
       "│         ┆            ┆ …          ┆ Li…        ┆ about…     ┆ …          ┆ ets…      ┆           │\n",
       "│ 3       ┆ [\"151088\"] ┆ [\"summer-2 ┆ Bratfest   ┆ Massie     ┆ https://ww ┆ https://i ┆ [{\"4605\", │\n",
       "│         ┆            ┆ 017\", \"bul ┆ at         ┆ Block: The ┆ w.goodread ┆ mages.gr- ┆ \"\"}]      │\n",
       "│         ┆            ┆ lying\", …  ┆ Tiffany's  ┆ Briarwood  ┆ s.com/book ┆ assets.co ┆           │\n",
       "│         ┆            ┆ …          ┆ (Clique …  ┆ bo…        ┆ …          ┆ m/b…      ┆           │\n",
       "│ 4       ┆ [\"812067\"] ┆ [\"rosemary ┆ Questions  ┆ When Sammi ┆ https://ww ┆ https://i ┆ [{\"792676 │\n",
       "│         ┆            ┆ -vernon\",  ┆ of Love    ┆ Edwards is ┆ w.goodread ┆ mages.gr- ┆ \",\"\"}]    │\n",
       "│         ┆            ┆ \"young-adu ┆ (Sweet     ┆ chosen t…  ┆ s.com/book ┆ assets.co ┆           │\n",
       "│         ┆            ┆ …          ┆ Dream…     ┆            ┆ …          ┆ m/b…      ┆           │\n",
       "└─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = pl.read_parquet(data_folder + \"books.pq\")\n",
    "print(\"Books:\")\n",
    "print(books.shape)\n",
    "display(books.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b1ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N books: 34322\n",
      "N 'cold' books: 3022\n"
     ]
    }
   ],
   "source": [
    "n_books = books.shape[0]\n",
    "print(f\"N books: {n_books}\")\n",
    "\n",
    "n_cold_books = n_books - train[\"item_id\"].n_unique()\n",
    "print(f\"N 'cold' books: {n_cold_books}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ae5c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:\n",
      "(185828, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>item_id</th></tr><tr><td>str</td><td>list[i64]</td></tr></thead><tbody><tr><td>&quot;00000377eea48021d3002730d56aca…</td><td>[13252]</td></tr><tr><td>&quot;00009ab2ed8cbfceda5a59da409663…</td><td>[2328]</td></tr><tr><td>&quot;00009e46d18f223a82b22da38586b6…</td><td>[28636, 30197]</td></tr><tr><td>&quot;0001085188e302fc6b2568de45a5f5…</td><td>[2159, 2969, … 33630]</td></tr><tr><td>&quot;00014c578111090720e20f5705eba0…</td><td>[45, 3513, … 33273]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬───────────────────────┐\n",
       "│ user_id                         ┆ item_id               │\n",
       "│ ---                             ┆ ---                   │\n",
       "│ str                             ┆ list[i64]             │\n",
       "╞═════════════════════════════════╪═══════════════════════╡\n",
       "│ 00000377eea48021d3002730d56aca… ┆ [13252]               │\n",
       "│ 00009ab2ed8cbfceda5a59da409663… ┆ [2328]                │\n",
       "│ 00009e46d18f223a82b22da38586b6… ┆ [28636, 30197]        │\n",
       "│ 0001085188e302fc6b2568de45a5f5… ┆ [2159, 2969, … 33630] │\n",
       "│ 00014c578111090720e20f5705eba0… ┆ [45, 3513, … 33273]   │\n",
       "└─────────────────────────────────┴───────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_exploded = pl.read_parquet(data_folder + \"test.pq\")\n",
    "test = test_exploded.group_by(\"user_id\", maintain_order=True).agg(pl.col(\"item_id\"))\n",
    "print(\"Test:\")\n",
    "print(test.shape)\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ba913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shape: (1, 1)\\n┌─────────────────────────────────┐\\n│ user_id                         │\\n│ ---                             │\\n│ str                             │\\n╞═════════════════════════════════╡\\n│ 00000377eea48021d3002730d56aca… │\\n└─────────────────────────────────┘'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.select('user_id')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13657f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHTUNG! DO NOT TOUCH\n",
    "\n",
    "@dataclass\n",
    "class AtKMetric(ABC):\n",
    "    k: int\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def full_name(self) -> str:\n",
    "        return f\"{self.name}@{self.k}\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, *, preds_col: str = \"preds\", ground_truth_col: str = \"ground_truth\") -> pl.Expr:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NDCG(AtKMetric):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"ndcg\"\n",
    "\n",
    "    def __call__(self, *, preds_col: str = \"preds\", ground_truth_col: str = \"ground_truth\") -> pl.Expr:\n",
    "        def _dcg(scores: np.ndarray) -> float:\n",
    "            # Handle empty arrays\n",
    "            if len(scores) == 0:\n",
    "                return 0.0\n",
    "            # Use vectorized operations for better performance\n",
    "            positions = np.arange(1, len(scores) + 1, dtype=np.float64)\n",
    "            return np.sum((np.power(2, scores) - 1) / np.log2(positions + 1))\n",
    "\n",
    "        def ndcg(predicted: list[t.Any], gt_items: list[t.Any]) -> float:\n",
    "            # Handle empty predictions or ground truth\n",
    "            if not predicted or not gt_items:\n",
    "                return 0.0\n",
    "                \n",
    "            # Take only top-k predictions\n",
    "            predicted = predicted[:self.k]\n",
    "            \n",
    "            # Create relevance scores (1 for relevant items, 0 otherwise)\n",
    "            relevance = np.array([1 if x in gt_items else 0 for x in predicted], dtype=np.float64)\n",
    "            \n",
    "            rank_dcg = _dcg(relevance)\n",
    "            if rank_dcg == 0.0:\n",
    "                return 0.0\n",
    "                \n",
    "            # Ideal DCG: sort ground truth by relevance (all 1s) and take top-k\n",
    "            ideal_relevance = np.ones(min(len(gt_items), self.k), dtype=np.float64)\n",
    "            ideal_dcg = _dcg(ideal_relevance)\n",
    "            \n",
    "            if ideal_dcg == 0.0:\n",
    "                return 0.0\n",
    "                \n",
    "            return rank_dcg / ideal_dcg\n",
    "\n",
    "        return pl.struct([preds_col, ground_truth_col]).map_elements(\n",
    "            lambda x: ndcg(x[preds_col], x[ground_truth_col]),\n",
    "            return_dtype=pl.Float64\n",
    "        ).alias(self.full_name)\n",
    "\n",
    "\n",
    "class Recall(AtKMetric):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"recall\"\n",
    "\n",
    "    def __call__(self, *, preds_col: str = \"preds\", ground_truth_col: str = \"ground_truth\") -> pl.Expr:\n",
    "        def recall(predicted: list[t.Any], gt_items: list[t.Any]) -> float:\n",
    "            # Handle empty ground truth\n",
    "            if not gt_items:\n",
    "                return 0.0\n",
    "                \n",
    "            # Take only top-k predictions\n",
    "            predicted = predicted[:self.k]\n",
    "            \n",
    "            # Calculate intersection\n",
    "            intersection = len(set(gt_items).intersection(set(predicted)))\n",
    "            return intersection / len(gt_items)\n",
    "\n",
    "        return pl.struct([preds_col, ground_truth_col]).map_elements(\n",
    "            lambda x: recall(x[preds_col], x[ground_truth_col]),\n",
    "            return_dtype=pl.Float64\n",
    "        ).alias(self.full_name)\n",
    "\n",
    "\n",
    "def coverage(df: pl.DataFrame, n_items_in_catalog: int, preds_col: str = \"preds\") -> float:\n",
    "    if n_items_in_catalog <= 0:\n",
    "        return 0.0\n",
    "        \n",
    "    unique_recommended = df.select(\n",
    "        pl.col(preds_col).explode().n_unique()\n",
    "    ).item()\n",
    "    return unique_recommended / n_items_in_catalog\n",
    "\n",
    "\n",
    "def evaluate_recommender(\n",
    "    df: pl.DataFrame,\n",
    "    model_preds_col: str,\n",
    "    ground_truth_col: str = \"item_id\",\n",
    "    n_items_in_catalog: int = n_books,\n",
    "    k: int = 10,\n",
    ") -> dict:\n",
    "    metrics = [\n",
    "        NDCG(k=k),\n",
    "        Recall(k=k),\n",
    "    ]\n",
    "    result = {}\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    result_df = df.clone()\n",
    "    \n",
    "    for metric in metrics:\n",
    "        result_df = result_df.with_columns(\n",
    "            metric(preds_col=model_preds_col, ground_truth_col=ground_truth_col)\n",
    "        )\n",
    "        result[metric.full_name] = round(result_df.select(pl.col(metric.full_name).mean()).item(), 3)\n",
    "\n",
    "    if n_items_in_catalog is not None:\n",
    "        result[\"coverage\"] = round(coverage(result_df, n_items_in_catalog, preds_col=model_preds_col), 3)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472747a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e869dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemkNN:\n",
    "    def __init__(self):\n",
    "        self.trained: bool = False\n",
    "        self.train_most_liked_item: pl.DataFrame | None = None\n",
    "        self.items_embeddings: np.ndarray | None = None\n",
    "        self.items_item_ids: np.ndarray | None = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        items_df: pl.DataFrame,\n",
    "        item_id_col: str = \"item_id\"\n",
    "    ) -> None:\n",
    "        self.items_embeddings = np.array(items_df[\"embedding\"].to_list())\n",
    "        self.items_item_ids = items_df[item_id_col].to_numpy()\n",
    "\n",
    "        # find the most liked item for each user\n",
    "        self.train_most_liked_item = (\n",
    "            df.lazy()\n",
    "            .sort(\"rating\", descending=True)\n",
    "            .group_by(\"user_id\")\n",
    "            .agg(pl.col(item_id_col).first().alias(item_id_col))\n",
    "            .join(\n",
    "                items_df.lazy().select([item_id_col, \"embedding\"]),\n",
    "                on=item_id_col,\n",
    "                how=\"left\"\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "        self.trained = True\n",
    "\n",
    "    def _get_batch_predictions(\n",
    "        self,\n",
    "        batch_user_embeddings: np.ndarray,\n",
    "        batch_current_items: np.ndarray,\n",
    "        batch_user_mask: np.ndarray,\n",
    "        topn: int\n",
    "    ) -> list[np.ndarray]:\n",
    "        batch_predictions = []\n",
    "        \n",
    "        # compute similarity scores for all items in batch\n",
    "        batch_scores = batch_user_embeddings @ self.items_embeddings.T\n",
    "\n",
    "        for user_scores, current_item, has_embedding in zip(\n",
    "            batch_scores, batch_current_items, batch_user_mask\n",
    "        ):\n",
    "            if not has_embedding:\n",
    "                batch_predictions.append([])\n",
    "                continue\n",
    "\n",
    "            # get top (topn + 1) most similar items\n",
    "            top_indices = np.argpartition(-user_scores, topn + 1)[:topn + 1]\n",
    "            \n",
    "            # filter out current item and take topn\n",
    "            recommended_items = []\n",
    "            for item_idx in top_indices:\n",
    "                if self.items_item_ids[item_idx] != current_item:\n",
    "                    recommended_items.append(self.items_item_ids[item_idx])\n",
    "                if len(recommended_items) >= topn:\n",
    "                    break\n",
    "\n",
    "            batch_predictions.append(recommended_items)\n",
    "\n",
    "        return batch_predictions\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        topn: int = 10,\n",
    "        batch_size: int = 5_000\n",
    "    ) -> list[np.ndarray]:\n",
    "        assert self.trained\n",
    "        \n",
    "        user_data = (\n",
    "            df.select([\"user_id\"])\n",
    "            .join(self.train_most_liked_item, on=\"user_id\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        user_embeddings = np.array(user_data[\"embedding\"].to_list())\n",
    "        current_items = user_data[\"item_id\"].to_numpy()\n",
    "        user_mask = user_data[\"embedding\"].is_not_null().to_numpy()\n",
    "\n",
    "        predictions = []\n",
    "        n_users = len(user_data)\n",
    "\n",
    "        for batch_start in tqdm(range(0, n_users, batch_size)):\n",
    "            batch_end = min(batch_start + batch_size, n_users)\n",
    "            \n",
    "            batch_predictions = self._get_batch_predictions(\n",
    "                user_embeddings[batch_start:batch_end],\n",
    "                current_items[batch_start:batch_end],\n",
    "                user_mask[batch_start:batch_end],\n",
    "                topn\n",
    "            )\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "item_knn = ItemkNN()\n",
    "item_knn.fit(train, books)\n",
    "test = test.with_columns(\n",
    "    item_knn_recs=pl.Series(item_knn.predict(test))\n",
    ")\n",
    "print(test.head())\n",
    "evaluate_recommender(df=test, model_preds_col=\"item_knn_recs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class  LinUCB:\n",
    "#     def __init__(self, alpha=1.0, d = 5, K_arms=10):\n",
    "#         self.trained = False\n",
    "#         self.alpha = alpha\n",
    "#         self.K_arms = K_arms\n",
    "#         self.linucb_arms = [self.linucb_disjoint_arm(i, d, alpha) for i in range(K_arms)]\n",
    "        \n",
    "# def inucb_disjoint_arm(self, arm_index, d, alpha, K_arms = 10):\n",
    "    \n",
    "\n",
    "#     @abstractmethod\n",
    "#     def fit(self, df: pl.DataFrame, **kwargs) -> None:\n",
    "#         # реализация может быть любой, никаких ограничений\n",
    "#         # не забудьте про self.trained = True\n",
    "#         self.trained = True\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def predict(self, df: pl.DataFrame, topn: int = 10, **kwargs) -> list[np.ndarray]:\n",
    "#         # реализация может быть любой, НО\n",
    "#         # должен возвращаться список массивов из item_id, которые есть в `books`, чтобы корректно работал подсчет метрик\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b4fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранная рука: 5\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class linucb_disjoint_arm:\n",
    "#     def __init__(self, arm_index, d, alpha, K_arms = 10):\n",
    "#         self.arm_index = arm_index\n",
    "#         self.alpha = alpha\n",
    "#         self.A = np.identity(d)\n",
    "#         self.b = np.zeros([d, 1])\n",
    "\n",
    "#     def calc_UCB(self, x_array):\n",
    "#         A_inv = np.linalg.inv(self.A)\n",
    "#         theta = np.dot(A_inv, self.b)\n",
    "#         x = x_array.reshape([-1, 1])\n",
    "#         p = np.dot(theta.T, x) + self.alpha * np.sqrt(np.dot(x.T, np.dot(A_inv, x)))\n",
    "#         return p\n",
    "\n",
    "#     def reward_update(self, reward, x_array):\n",
    "#         x = x_array.reshape([-1, 1])\n",
    "#         self.A += np.dot(x, x.T)\n",
    "#         self.b += reward * x\n",
    "\n",
    "# class linucb_policy:\n",
    "#     def __init__(self, K_arms, d, alpha):\n",
    "#         self.K_arms = K_arms\n",
    "#         self.linucb_arms = [linucb_disjoint_arm(i, d, alpha) for i in range(K_arms)]\n",
    "\n",
    "#     def select_arm(self, x_array):\n",
    "#         highest_ucb = -1\n",
    "#         candidate_arms = []\n",
    "\n",
    "#         for i in range(self.K_arms):\n",
    "#             ucb = self.linucb_arms[i].calc_UCB(x_array)\n",
    "#             if ucb > highest_ucb:\n",
    "#                 highest_ucb = ucb\n",
    "#                 candidate_arms = [i]\n",
    "#             elif ucb == highest_ucb:\n",
    "#                 candidate_arms.append(i)\n",
    "\n",
    "#         return np.random.choice(candidate_arms)\n",
    "\n",
    "# # пример использования\n",
    "# d = 5  # размерность контекста\n",
    "# K_arms = 10  # количество рук\n",
    "# alpha = 1.0  # параметр баланса исследования и использования\n",
    "# policy = linucb_policy(K_arms, d, alpha)\n",
    "\n",
    "# # допустим, у нас есть контекстный вектор для текущего пользователя\n",
    "# x_context = np.random.rand(d)\n",
    "\n",
    "# # выбор руки\n",
    "# chosen_arm = policy.select_arm(x_context)\n",
    "# print(\"Выбранная рука:\", chosen_arm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcac61d",
   "metadata": {},
   "source": [
    "## Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BaseRecommender(ABC):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, df: pl.DataFrame, **kwargs) -> None:\n",
    "        self.trained = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, df: pl.DataFrame, topn: int = 10, **kwargs) -> list[np.ndarray]:\n",
    "        pass\n",
    "\n",
    "class LinUCBRecommender(BaseRecommender):\n",
    "    def __init__(self, alpha: float = 1.0, context_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.context_dim = context_dim\n",
    "        self.policy: Optional[linucb_policy] = None\n",
    "        self.item_to_arm: Dict[int, int] = {}\n",
    "        self.arm_to_item: Dict[int, int] = {}\n",
    "        self.user_features: Optional[pl.DataFrame] = None\n",
    "        self.item_features: Optional[pl.DataFrame] = None\n",
    "        self.user_embeddings: Optional[Dict[int, np.ndarray]] = {}\n",
    "        self.item_embeddings: Optional[Dict[int, np.ndarray]] = {}\n",
    "        \n",
    "    def _create_context_vector(self, user_id: int, item_id: int = None) -> np.ndarray:\n",
    "        \"\"\"Create context vector from user and item features\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        # Add user embedding if available\n",
    "        if user_id in self.user_embeddings:\n",
    "            context_parts.append(self.user_embeddings[user_id])\n",
    "        else:\n",
    "            # Use zero vector if user not found\n",
    "            user_dim = len(next(iter(self.user_embeddings.values()))) if self.user_embeddings else 10\n",
    "            context_parts.append(np.zeros(user_dim))\n",
    "        \n",
    "        # Add item embedding if available and provided\n",
    "        if item_id is not None and item_id in self.item_embeddings:\n",
    "            context_parts.append(self.item_embeddings[item_id])\n",
    "        elif item_id is not None:\n",
    "            # Use zero vector if item not found\n",
    "            item_dim = len(next(iter(self.item_embeddings.values()))) if self.item_embeddings else 10\n",
    "            context_parts.append(np.zeros(item_dim))\n",
    "            \n",
    "        if context_parts:\n",
    "            return np.concatenate(context_parts)\n",
    "        else:\n",
    "            # Fallback to random context\n",
    "            return np.random.randn(self.context_dim or 20)\n",
    "\n",
    "    def fit(self, df: pl.DataFrame, \n",
    "            items_df: pl.DataFrame = None,\n",
    "            user_id_col: str = \"user_id\", \n",
    "            item_id_col: str = \"item_id\",\n",
    "            rating_col: str = \"rating\",\n",
    "            **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Fit the LinUCB model\n",
    "        \n",
    "        Args:\n",
    "            df: Training DataFrame with user_id, item_id, rating\n",
    "            items_df: Optional DataFrame with item features/embeddings\n",
    "            user_id_col: Column name for user ID\n",
    "            item_id_col: Column name for item ID  \n",
    "            rating_col: Column name for rating\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get unique items to create arms\n",
    "        unique_items = df[item_id_col].unique().sort()\n",
    "        self.item_to_arm = {item_id: idx for idx, item_id in enumerate(unique_items)}\n",
    "        self.arm_to_item = {idx: item_id for item_id, idx in self.item_to_arm.items()}\n",
    "        \n",
    "        # Extract embeddings if available\n",
    "        if items_df is not None and \"embedding\" in items_df.columns:\n",
    "            items_with_embeddings = items_df.filter(pl.col(\"embedding\").is_not_null())\n",
    "            for row in items_with_embeddings.iter_rows(named=True):\n",
    "                self.item_embeddings[row[item_id_col]] = np.array(row[\"embedding\"])\n",
    "        \n",
    "        # Create user embeddings based on their interaction patterns\n",
    "        # This is a simple approach - you might want to use more sophisticated user features\n",
    "        user_stats = (\n",
    "            df.group_by(user_id_col)\n",
    "            .agg([\n",
    "                pl.col(rating_col).mean().alias(\"avg_rating\"),\n",
    "                pl.col(rating_col).std().alias(\"rating_std\"),\n",
    "                pl.col(rating_col).count().alias(\"interaction_count\"),\n",
    "                pl.col(item_id_col).n_unique().alias(\"unique_items\")\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        for row in user_stats.iter_rows(named=True):\n",
    "            # Create simple user embedding from statistics\n",
    "            user_embedding = np.array([\n",
    "                row[\"avg_rating\"] or 0,\n",
    "                row[\"rating_std\"] or 0, \n",
    "                np.log(row[\"interaction_count\"] + 1),\n",
    "                np.log(row[\"unique_items\"] + 1)\n",
    "            ])\n",
    "            self.user_embeddings[row[user_id_col]] = user_embedding\n",
    "            \n",
    "        # Determine context dimension\n",
    "        sample_user = next(iter(self.user_embeddings.keys()))\n",
    "        sample_item = next(iter(self.item_embeddings.keys())) if self.item_embeddings else None\n",
    "        \n",
    "        sample_context = self._create_context_vector(sample_user, sample_item)\n",
    "        self.context_dim = len(sample_context)\n",
    "        \n",
    "        # Initialize LinUCB policy\n",
    "        n_arms = len(unique_items)\n",
    "        self.policy = linucb_policy(n_arms, self.context_dim, self.alpha)\n",
    "        \n",
    "        # Train on historical data\n",
    "        print(f\"Training LinUCB with {n_arms} items and context dimension {self.context_dim}\")\n",
    "        \n",
    "        # Convert ratings to binary rewards (1 if rating >= threshold, 0 otherwise)\n",
    "        rating_threshold = df[rating_col].median()\n",
    "        \n",
    "        for row in tqdm(df.iter_rows(named=True), desc=\"Training LinUCB\"):\n",
    "            user_id = row[user_id_col]\n",
    "            item_id = row[item_id_col]\n",
    "            rating = row[rating_col]\n",
    "            \n",
    "            if item_id in self.item_to_arm:\n",
    "                arm_idx = self.item_to_arm[item_id]\n",
    "                context = self._create_context_vector(user_id, item_id)\n",
    "                reward = 1.0 if rating >= rating_threshold else 0.0\n",
    "                \n",
    "                self.policy.linucb_arms[arm_idx].reward_update(reward, context)\n",
    "        \n",
    "        self.trained = True\n",
    "        print(f\"LinUCB training completed!\")\n",
    "\n",
    "    def predict(self, df: pl.DataFrame, topn: int = 10, \n",
    "                user_id_col: str = \"user_id\", **kwargs) -> list[list[int]]:\n",
    "        \"\"\"\n",
    "        Generate recommendations for users\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with user_id column\n",
    "            topn: Number of recommendations per user\n",
    "            user_id_col: Column name for user ID\n",
    "            \n",
    "        Returns:\n",
    "            List of recommendations (item_ids) for each user\n",
    "        \"\"\"\n",
    "        assert self.trained, \"Model must be trained before making predictions\"\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for row in tqdm(df.iter_rows(named=True), desc=\"Generating recommendations\"):\n",
    "            user_id = row[user_id_col]\n",
    "            \n",
    "            # Create context for this user (without specific item)\n",
    "            context = self._create_context_vector(user_id)\n",
    "            \n",
    "            # Get top N recommendations\n",
    "            top_arms = self.policy.get_top_arms(context, topn)\n",
    "            \n",
    "            # Convert arm indices back to item IDs\n",
    "            recommended_items = [self.arm_to_item[arm_idx] for arm_idx in top_arms]\n",
    "            predictions.append(recommended_items)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def update_reward(self, user_id: int, item_id: int, reward: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the model with new reward feedback\n",
    "        \n",
    "        Args:\n",
    "            user_id: User ID\n",
    "            item_id: Item ID that was recommended\n",
    "            reward: Reward received (e.g., 1 for click, 0 for no click)\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            return\n",
    "            \n",
    "        if item_id in self.item_to_arm:\n",
    "            arm_idx = self.item_to_arm[item_id]\n",
    "            context = self._create_context_vector(user_id, item_id)\n",
    "            self.policy.linucb_arms[arm_idx].reward_update(reward, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "recommender = LinUCBRecommender(alpha=1.0)\n",
    "\n",
    "# Train on historical data\n",
    "recommender.fit(train, books)\n",
    "\n",
    "# Generate recommendations\n",
    "test_with_recs = test.with_columns(\n",
    "    linucb_recs=pl.Series(recommender.predict(test, topn=10))\n",
    ")\n",
    "\n",
    "# Update with new feedback (online learning)\n",
    "recommender.update_reward(user_id=test.select('user_id')[0], item_id=456, reward=1.0)  # User liked the item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbce6af",
   "metadata": {},
   "source": [
    "## First level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridBookRecommender:\n",
    "    def __init__(self):\n",
    "        self.lightfm = LightFMModel()  # для теплых книг\n",
    "        self.cold_bandit = EmbeddingBasedBandit()\n",
    "        self.min_interactions_threshold = 5\n",
    "        \n",
    "    def classify_books(self, books_df, interactions_df):\n",
    "        \"\"\"Разделяем книги на холодные и теплые\"\"\"\n",
    "        interaction_counts = interactions_df.groupby('item_id').size()\n",
    "        \n",
    "        cold_books = books_df[\n",
    "            ~books_df['item_id'].isin(interaction_counts.index) |\n",
    "            books_df['item_id'].isin(\n",
    "                interaction_counts[interaction_counts < self.min_interactions_threshold].index\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        warm_books = books_df[\n",
    "            books_df['item_id'].isin(\n",
    "                interaction_counts[interaction_counts >= self.min_interactions_threshold].index\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return cold_books, warm_books\n",
    "    \n",
    "    def recommend(self, user_id, n_recommendations=10):\n",
    "        cold_books, warm_books = self.classify_books()\n",
    "        \n",
    "        # 20% - холодные, 80% - теплые\n",
    "        n_cold = max(1, int(n_recommendations * 0.2))\n",
    "        n_warm = n_recommendations - n_cold\n",
    "        \n",
    "        # Теплые рекомендации через LightFM\n",
    "        warm_recs = self.lightfm.recommend(user_id, n_warm)\n",
    "        \n",
    "        # Холодные через embeddings + бандиты\n",
    "        cold_recs = self.cold_bandit.recommend_cold_books(user_id, cold_books, n_cold)\n",
    "        \n",
    "        return warm_recs + cold_recs\n",
    "    \n",
    "    def update(self, user_id, item_id, reward):\n",
    "        \"\"\"Обновляем модели после получения фидбека\"\"\"\n",
    "        if self.is_cold_item(item_id):\n",
    "            self.cold_bandit.update(user_id, item_id, reward)\n",
    "            \n",
    "            # Проверяем, не пора ли перевести в теплые\n",
    "            if self.cold_bandit.book_stats[item_id]['n_interactions'] >= self.min_interactions_threshold:\n",
    "                self.move_to_warm(item_id)\n",
    "        else:\n",
    "            self.lightfm.update(user_id, item_id, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8b14d",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans2\n",
    "import random  # Для симуляции\n",
    "\n",
    "# Предполагаем DF загружен (твоя структура)\n",
    "# df = pl.read_parquet('books.parquet')  # Или твой способ загрузки\n",
    "# Для примера создадим маленький DF (замени на реальный)\n",
    "schema = {\n",
    "    'item_id': pl.Int64,\n",
    "    'series': pl.List(pl.String),\n",
    "    'tags': pl.List(pl.String),\n",
    "    'title': pl.String,\n",
    "    'description': pl.String,\n",
    "    'url': pl.String,\n",
    "    'image_url': pl.String,\n",
    "    'authors': pl.List(pl.Struct([pl.Field('id', pl.String), pl.Field('name', pl.String)])),\n",
    "    'title_embeddings': pl.Array(pl.Float32, 1024),\n",
    "    'image_embedding': pl.List(pl.Float64),\n",
    "    'description_embeddings': pl.Array(pl.Float32, 1024)  # Добавляем, как ты сказал\n",
    "}\n",
    "# Симуляция данных (в реальности удали)\n",
    "n_books = 34322\n",
    "embed_size = 1024\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        'item_id': range(n_books),\n",
    "        'series': [None] * n_books,\n",
    "        'tags': [[]] * n_books,\n",
    "        'title': ['Book ' + str(i)] * n_books,\n",
    "        'description': ['Desc ' + str(i)] * n_books,\n",
    "        'url': ['url'] * n_books,\n",
    "        'image_url': ['img'] * n_books,\n",
    "        'authors': [[{'id': '', 'name': ''}]] * n_books,\n",
    "        'title_embeddings': np.random.rand(n_books, embed_size).tolist(),\n",
    "        'image_embedding': np.random.rand(n_books, embed_size).tolist(),\n",
    "        'description_embeddings': np.random.rand(n_books, embed_size).tolist()\n",
    "    },\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Шаг 1: Объединение эмбеддингов (конкат title + description для 2048 dims)\n",
    "title_emb = np.stack(df['title_embeddings'].to_list())\n",
    "desc_emb = np.stack(df['description_embeddings'].to_list())\n",
    "# image_emb = np.stack(df['image_embedding'].to_list())  # Если хочешь добавить: np.concatenate([title_emb, desc_emb, image_emb], axis=1)\n",
    "embeddings = np.concatenate([title_emb, desc_emb], axis=1)  # Shape: (34322, 2048)\n",
    "\n",
    "# Кластеризация: 50 кластеров\n",
    "n_clusters = 50\n",
    "centroids, labels = kmeans2(embeddings, k=n_clusters, minit='points')\n",
    "df = df.with_columns(pl.Series('cluster', labels, dtype=pl.Int32))\n",
    "\n",
    "# Класс Epsilon-Greedy Bandit (как раньше)\n",
    "class EpsilonGreedyBandit:\n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        value = self.values[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "# Функция рекомендации 10 книг из кластера (сортировка по длине description как прокси; добавь rating если есть)\n",
    "def recommend_books(df, cluster, n=10):\n",
    "    cluster_books = df.filter(pl.col('cluster') == cluster)\n",
    "    # Сортировка: по длине description (длиннее = детальнее/популярнее?); или random\n",
    "    cluster_books = cluster_books.with_columns(pl.col('description').str.len_bytes().alias('desc_len'))\n",
    "    top_books = cluster_books.sort('desc_len', descending=True).head(n)\n",
    "    return top_books.select(['item_id', 'title', 'description', 'url'])\n",
    "\n",
    "# Инициализация для user N (отдельный бандит per user)\n",
    "bandit = EpsilonGreedyBandit(n_arms=n_clusters, epsilon=0.1)\n",
    "\n",
    "# Рекомендация для user N (холодный старт)\n",
    "cluster = bandit.select_arm()\n",
    "recommended = recommend_books(df, cluster)\n",
    "print(f\"Рекомендации для user N (кластер {cluster}):\")\n",
    "print(recommended)\n",
    "\n",
    "# Симуляция feedback (в реальности — от user: средний reward по 10 книгам, или per книга)\n",
    "reward = random.choice([0, 1])  # Эмуляция клика/покупки\n",
    "bandit.update(cluster, reward)\n",
    "print(f\"Reward: {reward}\")\n",
    "\n",
    "# Для нескольких сессий (обучение)\n",
    "for _ in range(100):  # Симуляция\n",
    "    cluster = bandit.select_arm()\n",
    "    reward = random.uniform(0, 1)  # В реальности на основе true prefs\n",
    "    bandit.update(cluster, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f403cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BaseRecommender(ABC):\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, df: pl.DataFrame, **kwargs) -> None:\n",
    "        self.trained = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, df: pl.DataFrame, topn: int = 10, **kwargs) -> list[np.ndarray]:\n",
    "        pass\n",
    "\n",
    "class BanditRecommender(BaseRecommender):\n",
    "    def __init__(self, n_clusters=50, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.epsilon = epsilon\n",
    "        self.item_ids = None\n",
    "        self.clusters = None\n",
    "        self.bandits = {}  # Храним бандитов для каждого user_id\n",
    "        self.books_df = None\n",
    "\n",
    "    class EpsilonGreedyBandit:\n",
    "        def __init__(self, n_arms, epsilon):\n",
    "            self.n_arms = n_arms\n",
    "            self.epsilon = epsilon\n",
    "            self.counts = np.zeros(n_arms)\n",
    "            self.values = np.zeros(n_arms)\n",
    "            \n",
    "        def select_arm(self):\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                return np.random.randint(0, self.n_arms)\n",
    "            return np.argmax(self.values)\n",
    "        \n",
    "        def update(self, arm, reward):\n",
    "            self.counts[arm] += 1\n",
    "            n = self.counts[arm]\n",
    "            value = self.values[arm]\n",
    "            self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "    def fit(self, df: pl.DataFrame, items_df: pl.DataFrame, item_id_col: str = \"item_id\", **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Кластеризуем книги по эмбеддингам и сохраняем данные для рекомендаций.\n",
    "        df: DataFrame с user_id (и, возможно, item_id, rating).\n",
    "        items_df: DataFrame с книгами (item_id, title_embeddings, description_embeddings).\n",
    "        \"\"\"\n",
    "        self.books_df = items_df\n",
    "        self.item_ids = items_df[item_id_col].to_numpy()\n",
    "\n",
    "        # Объединяем title_embeddings и description_embeddings\n",
    "        title_emb = np.stack(items_df['title_embeddings'].to_list())\n",
    "        desc_emb = np.stack(items_df['description_embeddings'].to_list())\n",
    "        embeddings = np.concatenate([title_emb, desc_emb], axis=1)  # Shape: (n_books, 2048)\n",
    "\n",
    "        # Кластеризация\n",
    "        _, labels = kmeans2(embeddings, k=self.n_clusters, minit='points')\n",
    "        self.clusters = labels  # Массив кластеров для каждого item_id\n",
    "\n",
    "        # Для каждого пользователя из df создаём бандита\n",
    "        for user_id in df['user_id'].unique():\n",
    "            self.bandits[user_id] = self.EpsilonGreedyBandit(n_arms=self.n_clusters, epsilon=self.epsilon)\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def _recommend_books(self, cluster: int, topn: int = 10) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Возвращает topn книг из указанного кластера.\n",
    "        Сортировка по длине description (как прокси популярности).\n",
    "        \"\"\"\n",
    "        cluster_books = self.books_df.filter(pl.col('cluster') == cluster)\n",
    "        cluster_books = cluster_books.with_columns(pl.col('description').str.len_bytes().alias('desc_len'))\n",
    "        top_books = cluster_books.sort('desc_len', descending=True).head(topn)\n",
    "        return top_books['item_id'].to_numpy()\n",
    "\n",
    "    def predict(self, df: pl.DataFrame, topn: int = 10, **kwargs) -> list[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Возвращает список рекомендаций (np.ndarray с item_id) для каждого user_id в df.\n",
    "        \"\"\"\n",
    "        assert self.trained, \"Model must be trained before prediction\"\n",
    "\n",
    "        # Добавляем кластеры в books_df\n",
    "        self.books_df = self.books_df.with_columns(pl.Series('cluster', self.clusters, dtype=pl.Int32))\n",
    "\n",
    "        predictions = []\n",
    "        for user_id in tqdm(df['user_id']):\n",
    "            # Получаем или создаём бандита для пользователя\n",
    "            if user_id not in self.bandits:\n",
    "                self.bandits[user_id] = self.EpsilonGreedyBandit(n_arms=self.n_clusters, epsilon=self.epsilon)\n",
    "            \n",
    "            # Выбираем кластер\n",
    "            cluster = self.bandits[user_id].select_arm()\n",
    "            \n",
    "            # Рекомендуем topn книг из кластера\n",
    "            recommended_items = self._recommend_books(cluster, topn)\n",
    "            \n",
    "            # Если меньше topn, дополняем случайными item_id\n",
    "            if len(recommended_items) < topn:\n",
    "                remaining = topn - len(recommended_items)\n",
    "                additional = np.random.choice(self.item_ids, size=remaining, replace=False)\n",
    "                recommended_items = np.concatenate([recommended_items, additional])\n",
    "            \n",
    "            predictions.append(recommended_items[:topn])\n",
    "\n",
    "            # Симуляция feedback (в проде заменить на реальный reward, например, через update метод)\n",
    "            reward = np.random.choice([0, 1])  # Эмуляция клика\n",
    "            self.bandits[user_id].update(cluster, reward)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация\n",
    "bandit_rec = BanditRecommender(n_clusters=50, epsilon=0.1)\n",
    "\n",
    "# Обучение (books — твой DataFrame с книгами, train — с user_id)\n",
    "bandit_rec.fit(train, books)\n",
    "\n",
    "# Предсказания\n",
    "test = test.with_columns(\n",
    "    bandit_recs=pl.Series(bandit_rec.predict(test, topn=10))\n",
    ")\n",
    "\n",
    "# Вывод первых строк\n",
    "print(test.head())\n",
    "\n",
    "# Оценка (предполагается, что evaluate_recommender определена)\n",
    "evaluate_recommender(df=test, model_preds_col=\"bandit_recs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
